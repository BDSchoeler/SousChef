<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title><%= htmlWebpackPlugin.options.title %></title>

  <meta name="description" content="<%= htmlWebpackPlugin.options.metadata.description %>">

  <% if (webpackConfig.htmlElements.headTags) { %>
  <!-- Configured Head Tags  -->
  <%= webpackConfig.htmlElements.headTags %>
  <% } %>

  <!-- base url -->
  <base href="<%= htmlWebpackPlugin.options.metadata.baseUrl %>">


</head>

<body>
<app>
</app>

<div id="preloader">
  <div></div>
</div>

<% if (htmlWebpackPlugin.options.metadata.isDevServer && htmlWebpackPlugin.options.metadata.HMR !== true) { %>
<!-- Webpack Dev Server reload -->
<script src="/webpack-dev-server.js"></script>
<% } %>

<script>
    var audioContext =  new initAudioContext();
    var audioPlayer = new AudioPlayer(audioContext);
    function initAudioContext()
    {
      var AudioContext = window.AudioContext || window.webkitAudioContext;
      if (!AudioContext)
      {
        throw "No WebAudio Support in this Browser";
      }
      navigator.getUserMedia = navigator.getUserMedia
          || navigator.webkitGetUserMedia
          || navigator.mozGetUserMedia
          || navigator.msGetUserMedia;
      if (!navigator.getUserMedia)
      {
        console.log("No getUserMedia Support in this Browser");
      }
      return new AudioContext();
    }

    function AudioPlayer(audioContext)
    {
      var isPlaying = false;
      var context = audioContext;
      var source = null;

      this.isPlaying = function ()
      {
        return isPlaying;
      };

      this.play = function (audio)
      {
        var audioToPlay = new Int16Array(audio);
        source = context.createBufferSource();
        var audioBuffer = context.createBuffer(1, audioToPlay.length, 8000);
        var channelData = audioBuffer.getChannelData(0);
        for (var i = 0; i < channelData.length; ++i)
        {
          channelData[i] = audioToPlay[i] / 32768.0;
        }
        source.buffer = audioBuffer;
        source.connect(context.destination);
        if (source.start) {
          source.start(0);
        }
        else {
          source.noteOn(0);
        }
        isPlaying = true;

        source.onended = function ()
        {
          isPlaying = false;
        };
      };

      this.stop = function ()
      {
        if (source != null) {
          source.stop();
          isPlaying = false;
        }
      }
    }


    /*
     *  The audio recorder uses promises (deferred object) from the Q.js library
     */
    function AudioRecorder(audioContext)
    {
      var context = audioContext;
      var mediaStream;
      var def;
      
      var desiredSampleRate = 8000;
      var audioInput;
      var analyserNode;
      var recordingNode;
      
      var resampler = new Resampler(context.sampleRate, desiredSampleRate, 1, 8192);

      this.start = function () {
        def = Q.defer();

        console.log("context.sampleRate = " + context.sampleRate);
        
        navigator.getUserMedia(
            
            {audio: true},
        
            function (stream) {
              mediaStream = stream;
              
              audioInput = context.createMediaStreamSource(stream);
              analyserNode = context.createAnalyser();
              recordingNode = context.createScriptProcessor(8192, 1, 2);
              recordingNode.onaudioprocess = function (evt) {

                var ch = resampler.resampler(evt.inputBuffer.getChannelData(0));

                var ampArray = new Uint8Array(analyserNode.frequencyBinCount);
                analyserNode.getByteTimeDomainData(ampArray);

                var encodedSpx = new Int16Array(ch.length);
                for (var i = 0; i < ch.length; ++i) {
                  var s = Math.max(-1, Math.min(1, ch[i]));
                  encodedSpx[i] = s <= -1.0 ? 0x8000 : (s >= 1.0 ? 0x7FFF : s * 0x8000);
                }

                def.notify([encodedSpx, ampArray]);
              };

              audioInput.connect(analyserNode);
              analyserNode.connect(recordingNode);
              recordingNode.connect(context.destination);
            },
            
            def.reject);

        return def.promise;
      };

      this.stop = function () {
        mediaStream.getTracks().forEach(function (track) {
          track.stop();
        });

        def.resolve();
      };
    }
  
    function tts () {
      var text = document.getElementById("text").value;
      console.log("tts being called");
      console.log(document.getElementById("text").value);
      if (text.length == 0) {
        return;
      }
      var url     = "https://pumpout.anyhowstep.com/tts/"+encodeURIComponent(text);
      var request = new XMLHttpRequest();
      request.open("GET", url, true);
      request.responseType = "arraybuffer";
      request.onreadystatechange = function (event) {
        console.log(request.status);
        if (request.readyState === 4 && request.status === 200) {
          var arrayBuffer = request.response;
          if (arrayBuffer) {
            audioPlayer.play(arrayBuffer);
          }
        }
      };
      request.send();
    }
</script>


<script>
    /*License (MIT)

    Copyright Â© 2013 Matt Diamond

    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated 
    documentation files (the "Software"), to deal in the Software without restriction, including without limitation 
    the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and 
    to permit persons to whom the Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all copies or substantial portions of 
    the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO 
    THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF 
    CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER 
    DEALINGS IN THE SOFTWARE.
    */

    (function(window){

      var WORKER_PATH = 'js/recorderjs/recorderWorker.js';

      var Recorder = function(source, cfg){
      var config = cfg || {};
      var bufferLen = config.bufferLen || 4096;
      this.context = source.context;
      if(!this.context.createScriptProcessor){
         this.node = this.context.createJavaScriptNode(bufferLen, 2, 2);
      } else {
         this.node = this.context.createScriptProcessor(bufferLen, 2, 2);
      }
       
      var worker = new Worker(config.workerPath || WORKER_PATH);
      worker.postMessage({
        command: 'init',
        config: {
        sampleRate: this.context.sampleRate
        }
      });
      var recording = false,
        currCallback;

      this.node.onaudioprocess = function(e){
        if (!recording) return;
        worker.postMessage({
        command: 'record',
        buffer: [
          e.inputBuffer.getChannelData(0),
          e.inputBuffer.getChannelData(1)
        ]
        });
      }

      this.configure = function(cfg){
        for (var prop in cfg){
        if (cfg.hasOwnProperty(prop)){
          config[prop] = cfg[prop];
        }
        }
      }
      
      this.isRecording = function () {
        return recording;
      }
      
      this.record = function(){
        recording = true;
      }

      this.stop = function(){
        recording = false;
      }

      this.clear = function(){
        worker.postMessage({ command: 'clear' });
      }

      this.getBuffers = function(cb) {
        currCallback = cb || config.callback;
        worker.postMessage({ command: 'getBuffers' })
      }

      this.exportWAV = function(cb, type){
        currCallback = cb || config.callback;
        type = type || config.type || 'audio/wav';
        if (!currCallback) throw new Error('Callback not set');
        worker.postMessage({
        command: 'exportWAV',
        type: type
        });
      }

      this.exportMonoWAV = function(cb, type){
        currCallback = cb || config.callback;
        type = type || config.type || 'audio/wav';
        if (!currCallback) throw new Error('Callback not set');
        worker.postMessage({
        command: 'exportMonoWAV',
        type: type
        });
      }

      worker.onmessage = function(e){
        var blob = e.data;
        currCallback(blob);
      }

      source.connect(this.node);
      this.node.connect(this.context.destination);   // if the script node is not connected to an output the "onaudioprocess" event is not triggered in chrome.
      };

      Recorder.setupDownload = function(blob, filename){
      var url = (window.URL || window.webkitURL).createObjectURL(blob);
      console.log(url);
      
      var data = new FormData();
      data.append("file", blob);
      
      var url = "https://pumpout.anyhowstep.com/voice";
      console.log(url);
      $.ajax({
        url:url,
        type:"POST",
        data:data,
        contentType: false,
        processData: false,
        success: function (data) {
          console.log(data);
          console.log(JSON.parse(data));
        }
      });
      }
      console.log(url);
      window.Recorder = Recorder;

    })(window);
  
    /* Copyright 2013 Chris Wilson

       Licensed under the Apache License, Version 2.0 (the "License");
       you may not use this file except in compliance with the License.
       You may obtain a copy of the License at

         http://www.apache.org/licenses/LICENSE-2.0

       Unless required by applicable law or agreed to in writing, software
       distributed under the License is distributed on an "AS IS" BASIS,
       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       See the License for the specific language governing permissions and
       limitations under the License.
    */

    window.AudioContext = window.AudioContext || window.webkitAudioContext;

    var audioContext = new AudioContext();
    var audioInput = null,
      realAudioInput = null,
      inputPoint = null,
      audioRecorder = null;
    var rafID = null;
    var analyserContext = null;
    var canvasWidth, canvasHeight;
    var recIndex = 0;

    /* TODO:

    - offer mono option
    - "Monitor input" switch
    */

    function saveAudio() {
      audioRecorder.exportWAV( doneEncoding );
      // could get mono instead by saying
      // audioRecorder.exportMonoWAV( doneEncoding );
    }

    function gotBuffers( buffers ) {
      //var canvas = document.getElementById( "wavedisplay" );

      //drawBuffer( canvas.width, canvas.height, canvas.getContext('2d'), buffers[0] );

      // the ONLY time gotBuffers is called is right after a new recording is completed - 
      // so heres where we should set up the download.
      audioRecorder.exportWAV( doneEncoding );
    }

    function doneEncoding( blob ) {
      Recorder.setupDownload( blob, "myRecording" + ((recIndex<10)?"0":"") + recIndex + ".wav" );
      recIndex++;
    }

    function toggleRecording( e ) {
      if (e.classList.contains("recording")) {
        // stop recording
        audioRecorder.stop();
        e.classList.remove("recording");
        audioRecorder.getBuffers( gotBuffers );
      } else {
        // start recording
        if (!audioRecorder)
          return;
        e.classList.add("recording");
        audioRecorder.clear();
        audioRecorder.record();
      }
    }

    function convertToMono( input ) {
      var splitter = audioContext.createChannelSplitter(2);
      var merger = audioContext.createChannelMerger(2);

      input.connect( splitter );
      splitter.connect( merger, 0, 0 );
      splitter.connect( merger, 0, 1 );
      return merger;
    }

    function cancelAnalyserUpdates() {
      window.cancelAnimationFrame( rafID );
      rafID = null;
    }

    function updateAnalysers(time) {
      if (!analyserContext) {
        var canvas = document.getElementById("analyser");
        canvasWidth = canvas.width;
        canvasHeight = canvas.height;
        analyserContext = canvas.getContext('2d');
      }

      // analyzer draw code here
      {
        var SPACING = 3;
        var BAR_WIDTH = 1;
        var numBars = Math.round(canvasWidth / SPACING);
        var freqByteData = new Uint8Array(analyserNode.frequencyBinCount);

        analyserNode.getByteFrequencyData(freqByteData); 

        analyserContext.clearRect(0, 0, canvasWidth, canvasHeight);
        analyserContext.fillStyle = '#F6D565';
        analyserContext.lineCap = 'round';
        var multiplier = analyserNode.frequencyBinCount / numBars;

        // Draw rectangle for each frequency bin.
        for (var i = 0; i < numBars; ++i) {
          var magnitude = 0;
          var offset = Math.floor( i * multiplier );
          // gotta sum/average the block, or we miss narrow-bandwidth spikes
          for (var j = 0; j< multiplier; j++)
            magnitude += freqByteData[offset + j];
          magnitude = magnitude / multiplier;
          var magnitude2 = freqByteData[i * multiplier];
          analyserContext.fillStyle = "hsl( " + Math.round((i*360)/numBars) + ", 100%, 50%)";
          analyserContext.fillRect(i * SPACING, canvasHeight, BAR_WIDTH, -magnitude);
        }
      }
      
      rafID = window.requestAnimationFrame( updateAnalysers );
    }

    function toggleMono() {
      if (audioInput != realAudioInput) {
        audioInput.disconnect();
        realAudioInput.disconnect();
        audioInput = realAudioInput;
      } else {
        realAudioInput.disconnect();
        audioInput = convertToMono( realAudioInput );
      }

      audioInput.connect(inputPoint);
    }

    function gotStream(stream) {
      inputPoint = audioContext.createGain();

      // Create an AudioNode from the stream.
      realAudioInput = audioContext.createMediaStreamSource(stream);
      audioInput = realAudioInput;
      audioInput.connect(inputPoint);

    //    audioInput = convertToMono( input );

      analyserNode = audioContext.createAnalyser();
      analyserNode.fftSize = 2048;
      inputPoint.connect( analyserNode );

      audioRecorder = new Recorder( inputPoint );

      zeroGain = audioContext.createGain();
      zeroGain.gain.value = 0.0;
      inputPoint.connect( zeroGain );
      zeroGain.connect( audioContext.destination );
      //updateAnalysers();
    }

    function initAudio() {
        if (!navigator.getUserMedia)
          navigator.getUserMedia = navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
        if (!navigator.cancelAnimationFrame)
          navigator.cancelAnimationFrame = navigator.webkitCancelAnimationFrame || navigator.mozCancelAnimationFrame;
        if (!navigator.requestAnimationFrame)
          navigator.requestAnimationFrame = navigator.webkitRequestAnimationFrame || navigator.mozRequestAnimationFrame;

      navigator.getUserMedia(
        {
          "audio": {
            "mandatory": {
              "googEchoCancellation": "false",
              "googAutoGainControl": "false",
              "googNoiseSuppression": "false",
              "googHighpassFilter": "false"
            },
            "optional": []
          },
        }, gotStream, function(e) {
          alert('Error getting audio');
          console.log(e);
        });
    }

    window.addEventListener('load', initAudio );
  
    function tryRecognize () {
      if (audioRecorder.isRecording()) {
        console.log("recording");
      } else {
        console.log("not recording");
      }
      
    }
  </script>
</body>
</html>
